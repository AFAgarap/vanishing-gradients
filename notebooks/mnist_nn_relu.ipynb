{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of Vanishing Gradients and its solutions\n",
    "# Copyright (C) 2019  Abien Fred Agarap, Joshua Cruzada\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards Understanding of the Vanishing Gradients Problem and its solutions\n",
    "===\n",
    "\n",
    "## Overview\n",
    "\n",
    "We explore the performance of a neural network with different activation functions.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "__version__ = '1.0.0'\n",
    "__author__ = 'Abien Fred Agarap, Joshua Raphaelle Cruzada'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "assert tf.__version__.startswith('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate GPU memory as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "We use synthetic dataset for training and testing deep neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use blobs, circles, and moons synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0612 10:13:14.373939 139936070940416 deprecation.py:323] From /home/abien_agarap/venv/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_features = train_features.astype(np.float32).reshape(-1, 784) / 255.\n",
    "train_features += tf.random.normal(train_features.shape, stddev=5e-2, mean=0.)\n",
    "test_features = test_features.astype(np.float32).reshape(-1, 784) / 255.\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "train_dataset = train_dataset.prefetch(train_features.shape[0] // batch_size)\n",
    "train_dataset = train_dataset.shuffle(batch_size * 2)\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "In this notebook, we write a deep neural network using the TensorFlow Subclassing API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=units, activation=tf.nn.relu)\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=units, activation=tf.nn.relu)\n",
    "        self.hidden_layer_3 = tf.keras.layers.Dense(units=units, activation=tf.nn.relu)\n",
    "        self.hidden_layer_4 = tf.keras.layers.Dense(units=units, activation=tf.nn.relu)\n",
    "        self.hidden_layer_5 = tf.keras.layers.Dense(units=units, activation=tf.nn.relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=num_classes)\n",
    "        self.optimizer = tf.optimizers.SGD(learning_rate=3e-4, momentum=9e-1)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input_features):\n",
    "        activations = self.hidden_layer_1(input_features)\n",
    "        activations = self.hidden_layer_2(activations)\n",
    "        activations = self.hidden_layer_3(activations)\n",
    "        activations = self.hidden_layer_4(activations)\n",
    "        activations = self.hidden_layer_5(activations)\n",
    "        output = self.output_layer(activations)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function for the model. For general applicability across the synthetic datasets, we shall use the softmax cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the train step for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss, batch_features, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_features)\n",
    "        train_loss = loss(logits=logits, labels=batch_labels)\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return train_loss, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a recorder of gradients for TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients(gradients, step):\n",
    "    for index, gradient in enumerate(gradients):\n",
    "        if len(gradient.shape) == 1:\n",
    "            tf.summary.histogram('histogram/{}-bias-grad'.format(index), gradient, step)\n",
    "        elif len(gradient.shape) != 1:\n",
    "            tf.summary.histogram('histogram/{}-weights-grad'.format(index), gradient, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, dataset, epochs=10):\n",
    "    \n",
    "    writer = tf.summary.create_file_writer('tmp/{}-relu'.format(time.asctime()))    \n",
    "    \n",
    "    with writer.as_default():\n",
    "        with tf.summary.record_if(True):\n",
    "            step = 0\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                epoch_accuracy = []\n",
    "                for batch_features, batch_labels in dataset:\n",
    "                    \n",
    "                    batch_loss, train_gradients = train_step(model, loss_fn, batch_features, batch_labels)\n",
    "                    \n",
    "                    accuracy = tf.metrics.Accuracy()\n",
    "                    accuracy(tf.argmax(model(batch_features), 1), tf.argmax(batch_labels, 1))\n",
    "\n",
    "                    epoch_loss += batch_loss\n",
    "                    epoch_accuracy.append(accuracy.result())\n",
    "                    plot_gradients(train_gradients, step)\n",
    "                    \n",
    "                    step += 1\n",
    "                    \n",
    "                epoch_loss = tf.reduce_mean(epoch_loss)\n",
    "                epoch_accuracy = tf.reduce_mean(epoch_accuracy)\n",
    "                \n",
    "                tf.summary.scalar('loss', epoch_loss, step=step)\n",
    "                tf.summary.scalar('accuracy', epoch_accuracy, step=step)\n",
    "\n",
    "                if epoch != 0 and (epoch + 1) % 100 == 0:\n",
    "                    print('Epoch {}/{}. Loss : {}, Accuracy : {}'.format(epoch + 1, epochs, epoch_loss, epoch_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units\n",
    "\n",
    "The ReLU function became a popular activation function as it solves the vanishing gradients problem by thresholding activations to 0 if they are less than or equal to 0.\n",
    "\n",
    "$$ relu(x) = \\max(0, x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a neural net class with `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(units=512, num_classes=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 5-layer neural net with `relu` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500. Loss : 8.138785362243652, Accuracy : 0.8499232530593872\n",
      "Epoch 200/500. Loss : 4.929592132568359, Accuracy : 0.8997279405593872\n",
      "Epoch 300/500. Loss : 4.034863471984863, Accuracy : 0.9174456000328064\n",
      "Epoch 400/500. Loss : 3.495504140853882, Accuracy : 0.9286412000656128\n",
      "Epoch 500/500. Loss : 3.095698595046997, Accuracy : 0.9365059733390808\n",
      "training time : 1530.5167181491852\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train(model, loss_fn, train_dataset, epochs)\n",
    "print('training time : {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the classification performance of the neural net. First, we set up the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(model(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy(np.argmax(predictions, 1), np.argmax(test_labels, 1))\n",
    "test_precision = precision(predictions, test_labels)\n",
    "test_recall = recall(predictions, test_labels)\n",
    "test_auc = auc(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9388999938964844\n",
      "Precision : 1.0\n",
      "Recall : 0.10000000149011612\n",
      "AUC : 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : {}\\nPrecision : {}\\nRecall : {}\\nAUC : {}'.format(test_accuracy.numpy(),\n",
    "                                                                    test_precision.numpy(),\n",
    "                                                                    test_recall.numpy(),\n",
    "                                                                    test_auc.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting trained model\n",
    "\n",
    "Let's export the trained model for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/relu/1', 'tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustscore import TrustScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_relu = TrustScore()\n",
    "ts_relu.fit(train_features.numpy(), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trust_score, _ = ts_relu.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.422535205594187\n"
     ]
    }
   ],
   "source": [
    "print(test_trust_score.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
