Avoiding the vanishing gradients problem by gradient self-augmentation
===

by Abien Fred Agarap, Joshua Raphaelle Cruzada, Gabrielle Marie Torres, Ralph Vincent Regalado, Charibeth Cheng, and Arnulfo Azcarraga, PhD

## Abstract

## Results

### Experiment Setup

### Varying activation functions

### Classification layer: Softmax vs. SVM

### Our approach: Gradient revival

## License
