{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of Vanishing Gradients and its solutions\n",
    "# Copyright (C) 2019  Abien Fred Agarap, Joshua Cruzada\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards Understanding of the Vanishing Gradients Problem and its solutions\n",
    "===\n",
    "\n",
    "## Overview\n",
    "\n",
    "We explore the performance of a neural network with different activation functions.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We install TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load TensorBoard for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard --logdir tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "In this notebook, we write deep neural network using the TensorFlow low-level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of Neural Network with different activations\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "__version__ = '1.0.0'\n",
    "__author__ = 'Abien Fred Agarap'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate GPU memory as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the class for a deep neural net by first writing a constructor to accept the parameters for building the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, layers, activation):\n",
    "        self.weights = []\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.activation = activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for initializing the learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(NeuralNet):\n",
    "    def initialize_params(self):\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.weights.append(tf.Variable(tf.random.normal([self.layers[layer], self.layers[layer - 1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the forward pass for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(NeuralNet):\n",
    "    def forward_prop(self, batch_features):\n",
    "        activations = []\n",
    "        linear_activations = []\n",
    "        activations.append(tf.transpose(batch_features))\n",
    "        for layer in range(1, self.num_layers):\n",
    "            linear_activations.append(tf.matmul(self.weights[layer - 1], activations[layer - 1]))\n",
    "            if layer != self.num_layers - 1:\n",
    "                if self.activation == 'relu':\n",
    "                    activations.append(tf.nn.relu(linear_activations[layer - 1]))\n",
    "                elif self.activation == 'sigmoid':\n",
    "                    activations.append(tf.nn.sigmoid(linear_activations[layer - 1]))\n",
    "                elif self.activation == 'tanh':\n",
    "                    activations.append(tf.nn.tanh(linear_activations[layer - 1]))\n",
    "                elif self.activation == 'leaky_relu':\n",
    "                    activations.append(tf.nn.leaky_relu(linear_activations[layer - 1]))\n",
    "                elif self.activation == 'swish':\n",
    "                    activations.append(tf.nn.sigmoid(linear_activations[layer - 1]) * linear_activations[layer - 1])\n",
    "        return tf.transpose(linear_activations[self.num_layers - 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the prediction function that uses the forward pass of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(NeuralNet):\n",
    "    @tf.function\n",
    "    def predict(self, batch_features):\n",
    "        logits = self.forward_prop(batch_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training loop for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(NeuralNet):\n",
    "    def train(self, dataset, epochs=10):\n",
    "        self.initialize_params()\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "        writer = tf.summary.create_file_writer('tmp')\n",
    "\n",
    "        with writer.as_default():\n",
    "            with tf.summary.record_if(True):\n",
    "                for epoch in range(epochs):\n",
    "                    epoch_loss = 0\n",
    "                    epoch_accuracy = []\n",
    "                    for step, (batch_features, batch_labels) in enumerate(dataset):\n",
    "                        with tf.GradientTape() as tape:\n",
    "                            logits = self.forward_prop(batch_features)\n",
    "                            batch_loss = tf.losses.categorical_crossentropy(batch_labels, logits, from_logits=True)\n",
    "                            batch_loss = tf.reduce_mean(batch_loss)\n",
    "                        gradients = tape.gradient(batch_loss, self.weights)\n",
    "                        optimizer.apply_gradients(zip(gradients, self.weights))\n",
    "\n",
    "                        accuracy = tf.metrics.Accuracy()\n",
    "                        accuracy(tf.argmax(self.predict(batch_features), 1), tf.argmax(batch_labels, 1))\n",
    "\n",
    "                        epoch_loss += batch_loss\n",
    "                        epoch_accuracy.append(accuracy.result())\n",
    "                    epoch_loss = np.mean(epoch_loss)\n",
    "                    epoch_accuracy = np.mean(epoch_accuracy)\n",
    "\n",
    "                    tf.summary.scalar('loss', epoch_loss, step=step)\n",
    "                    tf.summary.scalar('accuracy', epoch_accuracy, step=step)\n",
    "\n",
    "                    if epoch != 0 and (epoch + 1) % 10 == 0:\n",
    "                        print('Epoch {}/{}. Loss : {}, Accuracy : {}'.format(epoch + 1, epochs, epoch_loss, epoch_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "We use synthetic dataset for training and testing deep neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use blobs, circles, and moons synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_training_dataset, blobs_test_dataset = create_dataset(batch_size=batch_size, data='blobs', onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_training_dataset, circles_test_dataset = create_dataset(batch_size=batch_size, data='circles', onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_training_dataset, moons_test_dataset = create_dataset(batch_size=batch_size, data='moons', onehot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Activation Functions\n",
    "\n",
    "We vary the activation functions used for a deep neural network with 5 layers.\n",
    "\n",
    "### Hyperbolic Tangent\n",
    "\n",
    "$$ tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet([2, 16, 16, 16, 16, 16, 3], activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 5-layer neural net with `tanh` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100. Loss : 1.884418249130249, Accuracy : 0.9986185431480408\n",
      "Epoch 20/100. Loss : 0.5408930778503418, Accuracy : 0.9998570680618286\n",
      "Epoch 30/100. Loss : 0.20754793286323547, Accuracy : 1.0\n",
      "Epoch 40/100. Loss : 0.07831117510795593, Accuracy : 1.0\n",
      "Epoch 50/100. Loss : 0.03708554059267044, Accuracy : 1.0\n",
      "Epoch 60/100. Loss : 0.015668511390686035, Accuracy : 1.0\n",
      "Epoch 70/100. Loss : 0.006831524893641472, Accuracy : 1.0\n",
      "Epoch 80/100. Loss : 0.0035598259419202805, Accuracy : 1.0\n",
      "Epoch 90/100. Loss : 0.0019118780037388206, Accuracy : 1.0\n",
      "Epoch 100/100. Loss : 0.0010167760774493217, Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.train(blobs_training_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the classification performance of the neural net. First, we set up the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the classification performance on the test predictions from the deep neural net with hyperbolic tangent activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6827281, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_predictions = tf.nn.softmax(model.predict(blobs_test_dataset[0]))\n",
    "accuracy(tanh_predictions.numpy(), blobs_test_dataset[1])\n",
    "precision(tanh_predictions.numpy(), blobs_test_dataset[1])\n",
    "recall(tanh_predictions.numpy(), blobs_test_dataset[1])\n",
    "auc(tanh_predictions.numpy(), blobs_test_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.0\n",
      "Precision : 1.0\n",
      "Recall : 0.3333333432674408\n",
      "AUC : 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}\\nPrecision : {}\\nRecall : {}\\nAUC : {}'.format(accuracy.result(), precision.result(), recall.result(), auc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "$$ \\sigma(x) = \\dfrac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet([2, 16, 16, 16, 16, 16, 3], activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 5-layer neural net with `sigmoid` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100. Loss : 17.32961082458496, Accuracy : 1.0\n",
      "Epoch 20/100. Loss : 2.4970147609710693, Accuracy : 1.0\n",
      "Epoch 30/100. Loss : 0.878398060798645, Accuracy : 1.0\n",
      "Epoch 40/100. Loss : 0.4019172191619873, Accuracy : 1.0\n",
      "Epoch 50/100. Loss : 0.19001628458499908, Accuracy : 1.0\n",
      "Epoch 60/100. Loss : 0.08843421190977097, Accuracy : 1.0\n",
      "Epoch 70/100. Loss : 0.04012209549546242, Accuracy : 1.0\n",
      "Epoch 80/100. Loss : 0.017887214198708534, Accuracy : 1.0\n",
      "Epoch 90/100. Loss : 0.008018615655601025, Accuracy : 1.0\n",
      "Epoch 100/100. Loss : 0.0036467076279222965, Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.train(blobs_training_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the classification performance of the neural net. First, we set up the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the classification performance on the test predictions from the deep neural net with hyperbolic tangent activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10240921, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_predictions = tf.nn.softmax(model.predict(blobs_test_dataset[0]))\n",
    "accuracy(sigmoid_predictions.numpy(), blobs_test_dataset[1])\n",
    "precision(sigmoid_predictions.numpy(), blobs_test_dataset[1])\n",
    "recall(sigmoid_predictions.numpy(), blobs_test_dataset[1])\n",
    "auc(sigmoid_predictions.numpy(), blobs_test_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.0\n",
      "Precision : 1.0\n",
      "Recall : 0.3333333432674408\n",
      "AUC : 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}\\nPrecision : {}\\nRecall : {}\\nAUC : {}'.format(accuracy.result(), precision.result(), recall.result(), auc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units\n",
    "\n",
    "$$ relu(x) = max(0, x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet([2, 16, 16, 16, 16, 16, 3], activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 5-layer neural net with `relu` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100. Loss : 12.686515808105469, Accuracy : 0.9982850551605225\n",
      "Epoch 20/100. Loss : 0.998207151889801, Accuracy : 0.9998094439506531\n",
      "Epoch 30/100. Loss : 0.0068329269997775555, Accuracy : 1.0\n",
      "Epoch 40/100. Loss : 0.001769143738783896, Accuracy : 1.0\n",
      "Epoch 50/100. Loss : 0.000889322254806757, Accuracy : 1.0\n",
      "Epoch 60/100. Loss : 0.0005361908697523177, Accuracy : 1.0\n",
      "Epoch 70/100. Loss : 0.0003328489838168025, Accuracy : 1.0\n",
      "Epoch 80/100. Loss : 0.0002011979086091742, Accuracy : 1.0\n",
      "Epoch 90/100. Loss : 0.00011687181540764868, Accuracy : 1.0\n",
      "Epoch 100/100. Loss : 6.567626405740157e-05, Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.train(blobs_training_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the classification performance of the neural net. First, we set up the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the classification performance on the test predictions from the deep neural net with hyperbolic tangent activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3413641, shape=(), dtype=float32, numpy=0.81797624>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_predictions = tf.nn.softmax(model.predict(blobs_test_dataset[0]))\n",
    "accuracy(relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "precision(relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "recall(relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "auc(relu_predictions.numpy(), blobs_test_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.7976296544075012\n",
      "Precision : 1.0\n",
      "Recall : 0.6359525322914124\n",
      "AUC : 0.8179762363433838\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {}\\nPrecision : {}\\nRecall : {}\\nAUC : {}'.format(accuracy.result(), precision.result(), recall.result(), auc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "$$ relu(x) = max(0.02x, x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet([2, 16, 16, 16, 16, 16, 3], activation='leaky_relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the 5-layer neural net with `leaky_relu` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(blobs_training_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the classification performance of the neural net. First, we set up the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the classification performance on the test predictions from the deep neural net with hyperbolic tangent activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu_predictions = tf.nn.softmax(model.predict(blobs_test_dataset[0]))\n",
    "accuracy(leaky_relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "precision(leaky_relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "recall(leaky_relu_predictions.numpy(), blobs_test_dataset[1])\n",
    "auc(leaky_relu_predictions.numpy(), blobs_test_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy : {}\\nPrecision : {}\\nRecall : {}\\nAUC : {}'.format(accuracy.result(), precision.result(), recall.result(), auc.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
